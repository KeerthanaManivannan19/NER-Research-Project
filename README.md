Multi Domain Named Entity Recognition

The proposed system employs Bi-LSTM and Bi-LSTM-CRF architectures to perform multi-domain Named Entity Recognition (NER) by leveraging domain-specific entity probability distributions rather than relying on traditional keyword-based domain identification. Separate Bi-LSTM and Bi-LSTM-CRF models are trained for each domain, enabling the system to learn contextual, semantic, and structural characteristics unique to that domain’s entities. The Bi-LSTM model processes input sequences in both forward and backward directions, capturing dependencies from past and future words, which allows it to understand the context of each token within a sentence. The CRF layer on top of the Bi-LSTM further refines the output by modeling valid tag transitions, ensuring that the predicted sequence of entity labels follows consistent BMEOS patterns. During inference, the input text is processed through all domain models, and the predicted entity sequences are quantitatively evaluated using confidence-weighted tag distributions. The domain with the highest cumulative entity confidence is selected as the most probable classification. This approach offers a significant advantage over keyword matching, as deep learning models capture contextual dependencies, resolve ambiguities, and correctly interpret polysemous terms that may appear across multiple departments.

Although the experimental work in this study focuses on two domains (Sports and Crime), the framework is inherently scalable and can be extended to any number of domains such as finance, human resources, marketing, or customer service by training additional domain-specific NER models using appropriate annotated data. In real-world applications, this methodology can be deployed in enterprise environments to automatically route incoming customer complaints, service requests, or support tickets based on their underlying semantic structure. For example, texts containing entities related to “salary processing,” “attendance records,” or “leave approval” would be routed to the HR department, while messages referencing “invoice IDs,” “transaction failures,” or “credit approvals” would be directed to Finance. By integrating deep sequence modeling with domain-level probabilistic reasoning, the system provides a robust and scalable solution for intelligent multi-domain text classification and automated workflow management.

Furthermore, the use of Bi-LSTM and Bi-LSTM-CRF models offers a computationally efficient and cost-effective alternative to transformer-based architectures such as BERT or RoBERTa. While transformers typically require large-scale hardware resources, substantial memory consumption, and extensive pre-training, BiLSTM-based models achieve high accuracy with significantly fewer parameters and lower inference latency. The CRF layer further enhances sequence labeling performance by enforcing valid tag transitions without increasing model size dramatically. This lightweight nature makes Bi-LSTM and Bi-LSTM-CRF especially suitable for real-time enterprise deployments, low-resource environments, and domains where frequent model retraining is necessary. Thus, the proposed system not only ensures strong domain-aware NER performance but also maintains practical feasibility and operational efficiency compared to computationally expensive transformer models.
